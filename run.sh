
#!/bin/bash
set -e

echo "ğŸš€ Starting Daily ArXiv Update Workflow..."

# --- 1. ä½¿ç”¨åŒ—äº¬æ—¶åŒºè·å–æ­£ç¡®çš„å½“å¤©æ—¥æœŸ ---
today=$(TZ=Asia/Shanghai date "+%Y-%m-%d")
yesterday=$(TZ=Asia/Shanghai date -d "yesterday" "+%Y-%m-%d")
echo "âœ… Workflow date set to: ${today} (Asia/Shanghai)"

# --- 2. å®šä¹‰æ‰€æœ‰æ–‡ä»¶å ---
RAW_JSONL_FILE="data/${today}.jsonl"
UNIQUE_JSONL_FILE="data/${today}_unique.jsonl"
YESTERDAY_UNIQUE_FILE="data/${yesterday}_unique.jsonl"
ENHANCED_JSONL_FILE="data/${today}_unique_AI_enhanced_Chinese.jsonl"
FINAL_MD_FILE="data/${today}.md"

# --- 3. è¿è¡Œ Scrapy çˆ¬è™« ---
echo "--- Step 1: Crawling data from ArXiv ---"
(cd daily_arxiv && scrapy crawl arxiv -o ../${RAW_JSONL_FILE})
echo "âœ… Raw data saved to ${RAW_JSONL_FILE}"

# --- 4. è¿è¡Œå»é‡è„šæœ¬ ---
echo "--- Step 2: Deduplicating raw data ---"
python deduplicate.py ${RAW_JSONL_FILE} -o ${UNIQUE_JSONL_FILE}
echo "âœ… Unique data saved to ${UNIQUE_JSONL_FILE}"

# --- 5. æ–°å¢ï¼šè¿è¡Œå¢é‡è¿‡æ»¤è„šæœ¬  ---
echo "--- Step 2: Filtering for new items only ---"
# è°ƒç”¨æ–°è„šæœ¬ï¼Œæ¯”è¾ƒä»Šå¤©çš„æ–‡ä»¶å’Œè¿‡å»ä¸¤å¤©çš„æ–‡ä»¶
python filter_new.py --today ${UNIQUE_JSONL_FILE} --previous-days ${PREVIOUS_DAY_FILES[@]} --output ${NEW_ONLY_JSONL_FILE}

# --- 6. è¿è¡Œ AI å¢å¼ºè„šæœ¬ ---
echo "--- Step 4: Enhancing data with AI ---"
# ç¡®ä¿å®ƒçš„è¾“å…¥æ˜¯å»é‡åçš„æ–‡ä»¶
python ai/enhance.py --data ${UNIQUE_JSONL_FILE}
echo "âœ… AI enhancement complete. Output is ${ENHANCED_JSONL_FILE}"

# --- 7. è¿è¡Œ Markdown ç”Ÿæˆè„šæœ¬ ---
echo "--- Step 5: Converting JSONL to Markdown ---"
python to_md/convert.py --data ${ENHANCED_JSONL_FILE}
echo "âœ… Markdown report generated at ${FINAL_MD_FILE}"

# --- 8. æ›´æ–°ä¸» README æ–‡ä»¶ ---
echo "--- Step 6: Updating main README.md ---"
python update_readme.py
echo "âœ… README.md updated."

echo "ğŸ‰ Workflow finished successfully!"
è¯·å¸®æˆ‘ä¼˜åŒ–ä¸€ä¸‹ï¼Œæ¯”è¾ƒä»Šå¤©ä¸æ˜¨å¤©å’Œå‰å¤©çš„å†…å®¹å¦‚æœå­˜åœ¨é‡å¤è¿™åˆ å»ä»Šå¤©jsonlçš„è¯¥idæ‰€å±å­—æ®µ
