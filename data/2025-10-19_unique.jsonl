{"id": "2510.13886", "pdf": "https://arxiv.org/pdf/2510.13886", "abs": "https://arxiv.org/abs/2510.13886", "authors": ["Pierre Fayolle", "Alexandre B\u00f4ne", "No\u00eblie Debs", "Mathieu Naudin", "Pascal Bourdon", "Remy Guillevin", "David Helbert"], "title": "Physics-Informed autoencoder for DSC-MRI Perfusion post-processing: application to glioma grading", "categories": ["q-bio.QM", "cs.AI", "eess.IV", "eess.SP"], "comment": "5 pages, 5 figures, IEEE ISBI 2025, Houston, Tx, USA", "summary": "DSC-MRI perfusion is a medical imaging technique for diagnosing and\nprognosing brain tumors and strokes. Its analysis relies on mathematical\ndeconvolution, but noise or motion artifacts in a clinical environment can\ndisrupt this process, leading to incorrect estimate of perfusion parameters.\nAlthough deep learning approaches have shown promising results, their\ncalibration typically rely on third-party deconvolution algorithms to generate\nreference outputs and are bound to reproduce their limitations.\n  To adress this problem, we propose a physics-informed autoencoder that\nleverages an analytical model to decode the perfusion parameters and guide the\nlearning of the encoding network. This autoencoder is trained in a\nself-supervised fashion without any third-party software and its performance is\nevaluated on a database with glioma patients. Our method shows reliable results\nfor glioma grading in accordance with other well-known deconvolution algorithms\ndespite a lower computation time. It also achieved competitive performance even\nin the presence of high noise which is critical in a medical environment."}
{"id": "2510.13896", "pdf": "https://arxiv.org/pdf/2510.13896", "abs": "https://arxiv.org/abs/2510.13896", "authors": ["Xi Yu", "Yang Yang", "Qun Liu", "Yonghua Du", "Sean McSweeney", "Yuewei Lin"], "title": "GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "cs.MA"], "comment": "43 pages", "summary": "Cellular image segmentation is essential for quantitative biology yet remains\ndifficult due to heterogeneous modalities, morphological variability, and\nlimited annotations. We present GenCellAgent, a training-free multi-agent\nframework that orchestrates specialist segmenters and generalist\nvision-language models via a planner-executor-evaluator loop (choose tool\n$\\rightarrow$ run $\\rightarrow$ quality-check) with long-term memory. The\nsystem (i) automatically routes images to the best tool, (ii) adapts on the fly\nusing a few reference images when imaging conditions differ from what a tool\nexpects, (iii) supports text-guided segmentation of organelles not covered by\nexisting models, and (iv) commits expert edits to memory, enabling\nself-evolution and personalized workflows. Across four cell-segmentation\nbenchmarks, this routing yields a 15.7\\% mean accuracy gain over\nstate-of-the-art baselines. On endoplasmic reticulum and mitochondria from new\ndatasets, GenCellAgent improves average IoU by 37.6\\% over specialist models.\nIt also segments novel objects such as the Golgi apparatus via iterative\ntext-guided refinement, with light human correction further boosting\nperformance. Together, these capabilities provide a practical path to robust,\nadaptable cellular image segmentation without retraining, while reducing\nannotation burden and matching user preferences."}
{"id": "2510.13897", "pdf": "https://arxiv.org/pdf/2510.13897", "abs": "https://arxiv.org/abs/2510.13897", "authors": ["Naomi Fridman", "Anat Goldstein"], "title": "Dual-attention ResNet outperforms transformers in HER2 prediction on DCE-MRI", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Breast cancer is the most diagnosed cancer in women, with HER2 status\ncritically guiding treatment decisions. Noninvasive prediction of HER2 status\nfrom dynamic contrast-enhanced MRI (DCE-MRI) could streamline diagnostics and\nreduce reliance on biopsy. However, preprocessing high-dynamic-range DCE-MRI\ninto standardized 8-bit RGB format for pretrained neural networks is\nnontrivial, and normalization strategy significantly affects model performance.\nWe benchmarked intensity normalization strategies using a Triple-Head\nDual-Attention ResNet that processes RGB-fused temporal sequences from three\nDCE phases. Trained on a multicenter cohort (n=1,149) from the I-SPY trials and\nexternally validated on BreastDCEDL_AMBL (n=43 lesions), our model outperformed\ntransformer-based architectures, achieving 0.75 accuracy and 0.74 AUC on I-SPY\ntest data. N4 bias field correction slightly degraded performance. Without\nfine-tuning, external validation yielded 0.66 AUC, demonstrating\ncross-institutional generalizability. These findings highlight the\neffectiveness of dual-attention mechanisms in capturing transferable\nspatiotemporal features for HER2 stratification, advancing reproducible deep\nlearning biomarkers in breast cancer imaging."}
{"id": "2510.13911", "pdf": "https://arxiv.org/pdf/2510.13911", "abs": "https://arxiv.org/abs/2510.13911", "authors": ["Jia Zhang", "Bodong Du", "Yitong Miao", "Dongwei Sun", "Xiangyong Cao"], "title": "OralGPT: A Two-Stage Vision-Language Model for Oral Mucosal Disease Diagnosis and Description", "categories": ["q-bio.QM"], "comment": null, "summary": "Oral mucosal diseases such as leukoplakia, oral lichen planus, and recurrent\n  aphthous ulcers exhibit diverse and overlapping visual features,\n  making diagnosis challenging for non-specialists. While vision-language\n  models (VLMs) have shown promise in medical image interpretation,\n  their application in oral healthcare remains underexplored due to\n  the lack of large-scale, well-annotated datasets. In this work, we present\n  \\textbf{OralGPT}, the first domain-specific two-stage vision-language\n  framework designed for oral mucosal disease diagnosis and captioning.\n  In Stage 1, OralGPT learns visual representations and disease-related\n  concepts from classification labels. In Stage 2, it enhances its language\n  generation ability using long-form expert-authored captions. To\n  overcome the annotation bottleneck, we propose a novel similarity-guided\n  data augmentation strategy that propagates descriptive knowledge from\n  expert-labeled images to weakly labeled ones. We also construct the\n  first benchmark dataset for oral mucosal diseases, integrating multi-source\n  image data with both structured and unstructured textual annotations.\n  Experimental results on four common oral conditions demonstrate that\n  OralGPT achieves competitive diagnostic performance while generating\n  fluent, clinically meaningful image descriptions. This study\n  provides a foundation for language-assisted diagnostic tools in oral\n  healthcare."}
{"id": "2510.14282", "pdf": "https://arxiv.org/pdf/2510.14282", "abs": "https://arxiv.org/abs/2510.14282", "authors": ["Kazuya Horibe", "Daichi G. Suzuki"], "title": "Evolvable Chemotons: Toward the Integration of Autonomy and Evolution", "categories": ["q-bio.PE"], "comment": "Accepted as a late-breaking abstract in the ALIFE 2025", "summary": "In this study, we provide a relatively simple simulation framework for\nconstructing artificial life (ALife) with both autonomous and evolutionary\naspects by extending chemoton model. While the original chemoton incorporates\nmetabolism, membrane, and genetic templates, it lacks a mechanism for\nphenotypic variation, preventing true evolutionary dynamics. To address this,\nwe introduced a genotype-phenotype coupling by linking templates to a second\nautocatalytic cycle, enabling mutations to affect phenotype and be subject to\nselection. Using a genetic algorithm, we simulated populations of chemotons\nover generations. Results showed that chemotons without access to the new cycle\nremained in a stable but complexity-limited regime, while lineages acquiring\nthe additional metabolic set evolved longer templates. These findings\ndemonstrate that even simple replicator systems can achieve primitive\nevolvability, highlighting structural thresholds and rare innovations as key\ndrivers. Our framework provides a tractable model for exploring autonomy and\nevolution in ALife."}
{"id": "2510.13932", "pdf": "https://arxiv.org/pdf/2510.13932", "abs": "https://arxiv.org/abs/2510.13932", "authors": ["Henrik Pod\u00e9us", "Gustav Magnusson", "Sasan Keshmiri", "Kajsa Tunedal", "Nicolas Sundqvist", "William L\u00f6vfors", "Gunnar Cedersund"], "title": "SUND: simulation using nonlinear dynamic models - a toolbox for simulating multi-level, time-dynamic systems in a modular way", "categories": ["q-bio.QM", "65L05 (Primary) 37M05, 92C42 (Secondary)"], "comment": "6 pages, 1 figure, software paper. The last two listed authors\n  contributed equally to this work. Gunnar Cedersund is the corresponding\n  author", "summary": "When modeling complex, hierarchical, and time-dynamic systems, such as\nbiological systems, good computational tools are essential. Current tools,\nwhile powerful, often lack comprehensive frameworks for modular model\ncomposition, hierarchical system building, and time-dependent input handling,\nparticularly within the Python ecosystem. We present SUND (Simulation Using\nNonlinear Dynamic models), a Python toolbox designed to address these\nchallenges. SUND provides a unified framework for defining, combining, and\nsimulating multi-level time-dynamic systems. The toolbox enables users to\ndefine models with interconnectable inputs and outputs, facilitating the\nconstruction of complex systems from simpler, reusable components. It supports\ntime-dependent functions and piecewise constant inputs, enabling intuitive\nsimulation of various experimental conditions such as multiple dosing schemes.\nWe demonstrate the toolbox's capabilities through simulation of a multi-level\nhuman glucose-insulin system model, showcasing its flexibility in handling\nmultiple temporal scales, and levels of biological detail. SUND is open-source,\neasily extensible, and available at PyPI (https://pypi.org/project/sund/) and\nat Gitlab (https://gitlab.liu.se/ISBgroup/projects/sund/)."}
{"id": "2510.14481", "pdf": "https://arxiv.org/pdf/2510.14481", "abs": "https://arxiv.org/abs/2510.14481", "authors": ["Seong Jun Park"], "title": "Viral population dynamics at the cellular level, considering the replication cycle", "categories": ["q-bio.PE", "q-bio.QM"], "comment": null, "summary": "Viruses are microscopic infectious agents that require a host cell for\nreplication. Viral replication occurs in several stages, and the completion\ntime for each stage varies due to differences in the cellular environment.\nThus, the time to complete each stage in viral replication is a random\nvariable. However, no analytic expression exists for the viral population at\nthe cellular level when the completion time for each process constituting viral\nreplication is a random variable. This paper presents a simplified model of\nviral replication, treating each stage as a renewal process with independently\nand identically distributed completion times. Using the proposed model, we\nderive an analytical formula for viral populations at the cellular level, based\non viewing viral replication as a birth-death process. The mean viral count is\nexpressed via probability density functions representing the completion time\nfor each step in the replication process. This work validates the results with\nstochastic simulations. This study provides a new quantitative framework for\nunderstanding viral infection dynamics."}
{"id": "2510.14143", "pdf": "https://arxiv.org/pdf/2510.14143", "abs": "https://arxiv.org/abs/2510.14143", "authors": ["Alexandr A. Kalinin", "Anne E. Carpenter", "Shantanu Singh", "Matthew J. O'Meara"], "title": "cubic: CUDA-accelerated 3D Bioimage Computing", "categories": ["cs.CV", "q-bio.QM", "92C55, 68U10", "I.4.0; J.3"], "comment": "accepted to BioImage Computing workshop @ ICCV 2025", "summary": "Quantitative analysis of multidimensional biological images is useful for\nunderstanding complex cellular phenotypes and accelerating advances in\nbiomedical research. As modern microscopy generates ever-larger 2D and 3D\ndatasets, existing computational approaches are increasingly limited by their\nscalability, efficiency, and integration with modern scientific computing\nworkflows. Existing bioimage analysis tools often lack application programmable\ninterfaces (APIs), do not support graphics processing unit (GPU) acceleration,\nlack broad 3D image processing capabilities, and/or have poor interoperability\nfor compute-heavy workflows. Here, we introduce cubic, an open-source Python\nlibrary that addresses these challenges by augmenting widely used SciPy and\nscikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.\ncubic's API is device-agnostic and dispatches operations to GPU when data\nreside on the device and otherwise executes on CPU, seamlessly accelerating a\nbroad range of image processing routines. This approach enables GPU\nacceleration of existing bioimage analysis workflows, from preprocessing to\nsegmentation and feature extraction for 2D and 3D data. We evaluate cubic both\nby benchmarking individual operations and by reproducing existing deconvolution\nand segmentation pipelines, achieving substantial speedups while maintaining\nalgorithmic fidelity. These advances establish a robust foundation for\nscalable, reproducible bioimage analysis that integrates with the broader\nPython scientific computing ecosystem, including other GPU-accelerated methods,\nenabling both interactive exploration and automated high-throughput analysis\nworkflows. cubic is openly available at\nhttps://github$.$com/alxndrkalinin/cubic"}
{"id": "2510.14917", "pdf": "https://arxiv.org/pdf/2510.14917", "abs": "https://arxiv.org/abs/2510.14917", "authors": ["Hasan Ahmed", "Deena Goodgold", "Khushali Kothari", "Rustom Antia"], "title": "Cumulants, Moments and Selection: The Connection Between Evolution and Statistics", "categories": ["q-bio.PE"], "comment": null, "summary": "Cumulants and moments are closely related to the basic mathematics of\ncontinuous and discrete selection (respectively). These relationships\ngeneralize Fisher's fundamental theorem of natural selection and also make\nclear some of its limitation. The relationship between cumulants and continuous\nselection is especially intuitive and also provides an alternative way to\nunderstand cumulants. We show that a similarly simple relationship exists\nbetween moments and discrete selection. In more complex scenarios, we show that\nthinking of selection over discrete generations has significant advantages. For\na simple mutation model, we find exact solutions for the equilibrium moments of\nthe fitness distribution. These solutions are surprisingly simple and have some\ninteresting implications including: a necessary and sufficient condition for\nmutation selection balance, a very simple formula for mean fitness and the fact\nthat the shape of the equilibrium fitness distribution is determined solely by\nmutation (whereas the scale is determined by the starting fitness\ndistribution)."}
{"id": "2510.14188", "pdf": "https://arxiv.org/pdf/2510.14188", "abs": "https://arxiv.org/abs/2510.14188", "authors": ["Eric Albers", "Paul Marriott", "Masami Tatsuno"], "title": "Using Information Geometry to Characterize Higher-Order Interactions in EEG", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "In neuroscience, methods from information geometry (IG) have been\nsuccessfully applied in the modelling of binary vectors from spike train data,\nusing the orthogonal decomposition of the Kullback-Leibler divergence and\nmutual information to isolate different orders of interaction between neurons.\nWhile spike train data is well-approximated with a binary model, here we apply\nthese IG methods to data from electroencephalography (EEG), a continuous signal\nrequiring appropriate discretization strategies. We developed and compared\nthree different binarization methods and used them to identify third-order\ninteractions in an experiment involving imagined motor movements. The\nstatistical significance of these interactions was assessed using\nphase-randomized surrogate data that eliminated higher-order dependencies while\npreserving the spectral characteristics of the original signals. We validated\nour approach by implementing known second- and third-order dependencies in a\nforward model and quantified information attenuation at different steps of the\nanalysis. This revealed that the greatest loss in information occurred when\ngoing from the idealized binary case to enforcing these dependencies using\noscillatory signals. When applied to the real EEG dataset, our analysis\ndetected statistically significant third-order interactions during the task\ncondition despite the relatively sparse data (45 trials per condition). This\nwork demonstrates that IG methods can successfully extract genuine higher-order\ndependencies from continuous neural recordings when paired with appropriate\nbinarization schemes."}
{"id": "2510.14311", "pdf": "https://arxiv.org/pdf/2510.14311", "abs": "https://arxiv.org/abs/2510.14311", "authors": ["Ken-Ichi Nakamura", "Toshiko Ogiwara"], "title": "Propagation speed of traveling waves for diffusive Lotka-Volterra system with strong competition", "categories": ["math.AP", "q-bio.PE"], "comment": "15 pages, 2 figures", "summary": "We study the propagation speed of bistable traveling waves in the classical\ntwo-component diffusive Lotka-Volterra system under strong competition. From an\necological perspective, the sign of the propagation speed determines the\nlong-term outcome of competition between two species and thus plays a central\nrole in predicting the success or failure of invasion of an alien species into\nhabitats occupied by a native species. Using comparison arguments, we establish\nsufficient conditions determining the sign of the propagation speed, which\nrefine previously known results. In particular, we show that in the symmetric\ncase, where the two species differ only in their diffusion rates, the faster\ndiffuser prevails over a substantially broader parameter range than previously\nestablished. Moreover, we demonstrate that when the interspecific competition\ncoefficients differ significantly, the outcome of competition cannot be\nreversed by adjusting diffusion or growth rates. These findings provide a\nrigorous theoretical framework for analyzing invasion dynamics, offering\nsharper mathematical criteria for invasion success or failure."}
{"id": "2510.14787", "pdf": "https://arxiv.org/pdf/2510.14787", "abs": "https://arxiv.org/abs/2510.14787", "authors": ["Lorenzo Zino", "Alessandro Casu", "Alessandro Rizzo"], "title": "A Human-Vector Susceptible--Infected--Susceptible Model for Analyzing and Controlling the Spread of Vector-Borne Diseases", "categories": ["eess.SY", "cs.SY", "math.OC", "q-bio.PE"], "comment": "To appear in the Proceedings of the 2025 European Control Conference\n  (ECC)", "summary": "We propose an epidemic model for the spread of vector-borne diseases. The\nmodel, which is built extending the classical susceptible-infected-susceptible\nmodel, accounts for two populations -- humans and vectors -- and for\ncross-contagion between the two species, whereby humans become infected upon\ninteraction with carrier vectors, and vectors become carriers after interaction\nwith infected humans. We formulate the model as a system of ordinary\ndifferential equations and leverage monotone systems theory to rigorously\ncharacterize the epidemic dynamics. Specifically, we characterize the global\nasymptotic behavior of the disease, determining conditions for quick\neradication of the disease (i.e., for which all trajectories converge to a\ndisease-free equilibrium), or convergence to a (unique) endemic equilibrium.\nThen, we incorporate two control actions: namely, vector control and incentives\nto adopt protection measures. Using the derived mathematical tools, we assess\nthe impact of these two control actions and determine the optimal control\npolicy."}
